---
marp: true
---
## Google File System（GFS）的开源实现：HDFS

Google 大数据“三驾马车”的第一驾是 GFS（Google 文件系统），而 Hadoop 的第一个产品是 HDFS，可以说分布式文件存储是分布式计算的基础，也可见分布式文件存储的重要性。

HDFS 是在一个大规模分布式服务器集群上，对数据分片后进行并行读写及冗余存储。

---
![图片](https://mmbiz.qpic.cn/mmbiz_png/Wc4rFKffLwTMibeSvPBz7YWd591XhpZtib2XPyCqb8I9K7UQIKCqb7MAo2ISncvPxoIXOGpYtyBYz0zGvKdcdoRw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从图中你可以看到 HDFS 的关键组件有两个，一个是 DataNode，一个是 NameNode。

---


**DataNode 负责文件数据的存储和读写操作，HDFS 将文件数据分割成若干数据块（Block），每个 DataNode 存储一部分数据块，这样文件就分布存储在整个 HDFS 服务器集群中。**应用程序客户端（Client）可以并行对这些数据块进行访问，从而使得 HDFS 可以在服务器集群规模上实现数据并行访问，极大地提高了访问速度。

---

**NameNode 负责整个分布式文件系统的元数据（MetaData）管理，也就是文件路径名、数据块的 ID 以及存储位置等信息，相当于操作系统中文件分配表（FAT）的角色。**HDFS 为了保证数据的高可用，会将一个数据块复制为多份（缺省情况为 3 份），并将多份相同的数据块存储在不同的服务器上，甚至不同的机架上。这样当有磁盘损坏，或者某个 DataNode 服务器宕机，甚至某个交换机宕机，导致其存储的数据块不能访问的时候，客户端会查找其备份的数据块进行访问。

---
放大一下看数据块多份复制存储的实现。图中对于文件/users/sameerp/data/part-0，其复制备份数设置为 2，存储的 BlockID 分别为 1、3。Block1 的两个备份存储在 DataNode0 和 DataNode2 两个服务器上，Block3 的两个备份存储 DataNode4 和 DataNode6 两个服务器上，上述任何一台服务器宕机后，每个数据块都至少还有一个备份存在，不会影响对件/users/sameerp/data/part-0 的访问。

---

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/Wc4rFKffLwTMibeSvPBz7YWd591XhpZtibT3mOCFJEZ47OMSzB9AUUCfk35q7yAnjDd5ZBMw6K7KrK44A3fTyopQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)


和 RAID 一样，数据分成若干数据块后存储到不同服务器上，可以实现数据大容量存储，并且不同分片的数据可以并行进行读/写操作，进而实现数据的高速访问。

---

## MapReduce 的开源实现：Hadoop MapReduce

在我看来，**MapReduce 既是一个编程模型，又是一个计算框架。**也就是说，开发人员必须基于 MapReduce 编程模型进行编程开发，然后将程序通过 MapReduce 计算框架分发到 Hadoop 集群中运行。我们先看一下作为编程模型的 MapReduce。



举个 WordCount 的例子，WordCount 主要解决的是文本处理中词频统计的问题，就是统计文本中每一个单词出现的次数。MapReduce 版本 WordCount 程序的核心是一个 map 函数和一个 reduce 函数。

---

map 函数的输入主要是一个<Key, Value>对，在这个例子里，Value 是要统计的所有文本中的一行数据，Key 在一般计算中都不会用到。


map 函数的计算过程是，将这行文本中的单词提取出来，针对每个单词输出一个<word, 1>这样的<Key, Value>对。

MapReduce 计算框架会将这些<word , 1>收集起来，将相同的 word 放在一起，形成<word , <1,1,1,1,1,1,1…>>这样的<Key, Value 集合>数据，然后将其输入给 reduce 函数。

---
这里 reduce 的输入参数 Values 就是由很多个 1 组成的集合，而 Key 就是具体的单词 word。

reduce 函数的计算过程是，将这个集合里的 1 求和，再将单词（word）和这个和（sum）组成一个<Key, Value>，也就是<word, sum>输出。每一个输出就是一个单词和它的词频统计总和。

一个 map 函数可以针对一部分数据进行运算，这样就可以将一个大数据切分成很多块（这也正是 HDFS 所做的），MapReduce 计算框架为每个数据块分配一个 map 函数去计算，从而实现大数据的分布式计算。

---

![图片](https://mmbiz.qpic.cn/mmbiz_png/Wc4rFKffLwTMibeSvPBz7YWd591XhpZtibs7BJlMYkxcaGsl9c8ibs9naRibbzIWxkP1jgxyeTZVoUV9bcVltfUSbg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

---
接下来我们来看作为计算框架，MapReduce 是如何运作的。

以 Hadoop 1 为例，MapReduce 运行过程涉及三类关键进程。

- 大数据应用进程。这类进程是启动 MapReduce 程序的主入口，主要是指定 Map 和 Reduce 类、输入输出文件路径等，并提交作业给 Hadoop 集群，也就是下面提到的 JobTracker 进程。这是由用户启动的 MapReduce 程序进程，比如 WordCount 程序。
- JobTracker 进程。这类进程根据要处理的输入数据量，命令下面提到的 TaskTracker 进程启动相应数量的 Map 和 Reduce 进程任务，并管理整个作业生命周期的任务调度和监控。这是 Hadoop 集群的常驻进程，需要注意的是，JobTracker 进程在整个 Hadoop 集群全局唯一。
- TaskTracker 进程。这个进程负责启动和管理 Map 进程以及 Reduce 进程。因为需要每个数据块都有对应的 map 函数，TaskTracker 进程通常和 HDFS 的 DataNode 进程启动在同一个服务器。也就是说，Hadoop 集群中绝大多数服务器同时运行 DataNode 进程和 TaskTacker 进程。

---
- ![图片](https://mmbiz.qpic.cn/mmbiz_png/Wc4rFKffLwTMibeSvPBz7YWd591XhpZtibwneeupsPNuatKTia1ETyEo0xN5aZML2xkvuQ8oQ0uClVOp8TEEpJm2A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

查看MapReduce启动和运行的流程

---
**MapReduce 计算真正产生奇迹的地方是数据的合并与连接。**

还是回到 WordCount 例子中，我们想要统计相同单词在所有输入数据中出现的次数，而一个 Map 只能处理一部分数据，一个热门单词几乎会出现在所有的 Map 中，这意味着同一个单词必须要合并到一起进行统计才能得到正确的结果。

在 map 输出与 reduce 输入之间，MapReduce 计算框架处理数据合并与连接操作，这个操作有个专门的词汇叫 shuffle。那到底什么是 shuffle？shuffle 的具体过程又是怎样的呢？请看下图。

---

![图片](https://mmbiz.qpic.cn/mmbiz_png/Wc4rFKffLwTMibeSvPBz7YWd591XhpZtibSF4iaQXNUh70Pudt7fvUKMDE2vhTgSYIa07eHeHpZWVZNIT9icOvJlkA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

---
每个 Map 任务的计算结果都会写入到本地文件系统，等 Map 任务快要计算完成的时候，MapReduce 计算框架会启动 shuffle 过程，在 Map 任务进程调用一个 Partitioner 接口，对 Map 产生的每个<Key, Value>进行 Reduce 分区选择，然后通过 HTTP 通信发送给对应的 Reduce 进程。这样不管 Map 位于哪个服务器节点，相同的 Key 一定会被发送给相同的 Reduce 进程。Reduce 任务进程对收到的<Key, Value>进行排序和合并，相同的 Key 放在一起，组成一个<Key, Value 集合>传递给 Reduce 执行。

---
map 输出的<Key, Value>shuffle 到哪个 Reduce 进程是这里的关键，它是由 Partitioner 来实现，MapReduce 框架默认的 Partitioner 用 Key 的哈希值对 Reduce 任务数量取模，相同的 Key 一定会落在相同的 Reduce 任务 ID 上。

讲了这么多，对 shuffle 的理解，你只需要记住这一点：**分布式计算需要将不同服务器上的相关数据合并到一起进行下一步计算，这就是 shuffle。**

---
## BigTable 的开源实现：HBase

HBase 为可伸缩海量数据储存而设计，实现面向在线业务的实时数据访问延迟。HBase 的伸缩性主要依赖其可分裂的 HRegion 及可伸缩的分布式文件系统 HDFS 实现。
![图片](https://mmbiz.qpic.cn/mmbiz_png/Wc4rFKffLwTMibeSvPBz7YWd591XhpZtibScyB6DGVFcdssJgxWD2NYSQZOxasyB8gR3Mlq32vdc1MfwnGSpiaxUA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

---
HRegion 是 HBase 负责数据存储的主要进程，应用程序对数据的读写操作都是通过和 HRetion 通信完成。上面是 HBase 架构图，我们可以看到在 HBase 中，数据以 HRegion 为单位进行管理，也就是说应用程序如果想要访问一个数据，必须先找到 HRegion，然后将数据读写操作提交给 HRegion，由 HRegion 完成存储层面的数据操作。

HRegionServer 是物理服务器，每个 HRegionServer 上可以启动多个 HRegion 实例。当一个 HRegion 中写入的数据太多，达到配置的阈值时，一个 HRegion 会分裂成两个 HRegion，并将 HRegion 在整个集群中进行迁移，以使 HRegionServer 的负载均衡。

---
每个 HRegion 中存储一段 Key 值区间[key1, key2)的数据，所有 HRegion 的信息，包括存储的 Key 值区间、所在 HRegionServer 地址、访问端口号等，都记录在 HMaster 服务器上。为了保证 HMaster 的高可用，HBase 会启动多个 HMaster，并通过 ZooKeeper 选举出一个主服务器。

下面是一张调用时序图，应用程序通过 ZooKeeper 获得主 HMaster 的地址，输入 Key 值获得这个 Key 所在的 HRegionServer 地址，然后请求 HRegionServer 上的 HRegion，获得所需要的数据。

---

![图片](https://mmbiz.qpic.cn/mmbiz_png/Wc4rFKffLwTMibeSvPBz7YWd591XhpZtibBrNGicJ5g3l20lk5nr3xPXIro4DfauyR8kF7MyDsxubBM4E5VsCkmAg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

---
数据写入过程也是一样，需要先得到 HRegion 才能继续操作。HRegion 会把数据存储在若干个 HFile 格式的文件中，这些文件使用 HDFS 分布式文件系统存储，在整个集群内分布并高可用。当一个 HRegion 中数据量太多时，这个 HRegion 连同 HFile 会分裂成两个 HRegion，并根据集群中服务器负载进行迁移。如果集群中有新加入的服务器，也就是说有了新的 HRegionServer，由于其负载较低，也会把 HRegion 迁移过去并记录到 HMaster，从而实现 HBase 的线性伸缩。
